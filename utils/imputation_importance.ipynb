{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f254fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tldextract import extract\n",
    "from statistics import mode\n",
    "from tabulate import tabulate\n",
    "from retrieval_importance import learn_importance, encode_retrievals, encode_groups, v_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675a9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrievals(trainset, target, test_or_valid_set, test_or_valid_answers):\n",
    "    \n",
    "    retrievals = []\n",
    "\n",
    "    observed_labels = [label.lower() for label in trainset[target].unique()]\n",
    "\n",
    "    for sample_index, row in test_or_valid_set.iterrows():\n",
    "\n",
    "        retrieved_websites = []\n",
    "        generated_answers = []\n",
    "\n",
    "        raw_answers_for_sample = test_or_valid_answers[test_or_valid_answers.sample_index==sample_index]\\\n",
    "            .sort_values(by=['position'])\n",
    "\n",
    "        for _, raw_answer in raw_answers_for_sample.iterrows():\n",
    "\n",
    "            prediction = raw_answer.answer.lower()\n",
    "            # use training label if predicted label is a substring\n",
    "            for observed_label in observed_labels:\n",
    "                if prediction in observed_label:\n",
    "                    prediction = observed_label\n",
    "                    break        \n",
    "\n",
    "            retrieved_websites.append(raw_answer.url)\n",
    "            generated_answers.append(prediction)\n",
    "\n",
    "        retrievals.append({\n",
    "            \"sample_index\": sample_index,\n",
    "            \"correct_answers\": [row[target].lower()],\n",
    "            \"retrieved_websites\": retrieved_websites,\n",
    "            \"generated_answers\": generated_answers,\n",
    "        })    \n",
    "        \n",
    "    return retrievals        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e8a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(retrieval, prediction):\n",
    "    if prediction in retrieval[\"correct_answers\"]:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219b1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(retrieved):    \n",
    "    _, domain_name, ending = extract(retrieved)\n",
    "    return f'{domain_name}.{ending}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b51dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(retrievals, k):\n",
    "    correct = 0\n",
    "\n",
    "    for retrieval in retrievals:\n",
    "        prediction = mode(retrieval['generated_answers'][:k])\n",
    "        if prediction in retrieval['correct_answers']:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(retrievals)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def eval_accuracy_thresholded(retrievals, group_importances, k, threshold, keep_unknown_sources):\n",
    "    correct = 0\n",
    "\n",
    "    for retrieval in retrievals:\n",
    "\n",
    "        if keep_unknown_sources:\n",
    "            filtered_answers = [\n",
    "                answer\n",
    "                    for website, answer \n",
    "                    in zip(retrieval['retrieved_websites'], retrieval['generated_answers'])\n",
    "                    if not group(website) in group_importances or group_importances[group(website)] > threshold\n",
    "                ]\n",
    "        else:\n",
    "            filtered_answers = [\n",
    "                answer\n",
    "                    for website, answer \n",
    "                    in zip(retrieval['retrieved_websites'], retrieval['generated_answers'])\n",
    "                    if group(website) in group_importances and group_importances[group(website)] > threshold            \n",
    "                ]            \n",
    "            \n",
    "        if len(filtered_answers) > 0:\n",
    "            prediction = mode(filtered_answers[:k])\n",
    "            if prediction in retrieval['correct_answers']:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / len(retrievals)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6603b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset, target, k, keep_unknown_sources):\n",
    "\n",
    "    trainset = pd.read_csv(f'applications/imputation/data/{dataset}/train.csv')\n",
    "    validset = pd.read_csv(f'applications/imputation/data/{dataset}/valid.csv')\n",
    "    testset = pd.read_csv(f'applications/imputation/data/{dataset}/test.csv')\n",
    "\n",
    "    valid_answers = pd.read_csv(f'applications/imputation/answers/{dataset}_minilm/valid.csv')\n",
    "    test_answers = pd.read_csv(f'applications/imputation/answers/{dataset}_minilm/test.csv')\n",
    "    \n",
    "    validset_retrievals = create_retrievals(trainset, target, validset, valid_answers)\n",
    "    encoded_retrievals, mapping = encode_retrievals(validset_retrievals, \"retrieved_websites\", \n",
    "                                                    \"generated_answers\", utility)\n",
    "    grouping, group_mapping = encode_groups(mapping, group)\n",
    "\n",
    "    v = learn_importance(encoded_retrievals, k=k, learning_rate=0.1, num_steps=500, n_jobs=-1, grouping=grouping)\n",
    "\n",
    "    group_importances = v_grouped(v, grouping, group_mapping)    \n",
    "    \n",
    "    validation_accuracy = eval_accuracy(validset_retrievals, k=k)\n",
    "    validation_accuracy_thresholded = eval_accuracy_thresholded(validset_retrievals, group_importances, \n",
    "        k=k, threshold=0.5, keep_unknown_sources=keep_unknown_sources)\n",
    "\n",
    "    testset_retrievals = create_retrievals(trainset, target, testset, test_answers)    \n",
    "    \n",
    "    test_accuracy = eval_accuracy(testset_retrievals, k=k)\n",
    "    test_accuracy_thresholded = eval_accuracy_thresholded(testset_retrievals, group_importances, \n",
    "        k=k, threshold=0.5, keep_unknown_sources=keep_unknown_sources)    \n",
    "    \n",
    "    return validation_accuracy, validation_accuracy_thresholded, test_accuracy, test_accuracy_thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa66b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_buy, val_acc_clean_buy, test_acc_buy, test_acc_clean_buy = run_experiment(\n",
    "    dataset = 'buy',\n",
    "    target = 'manufacturer',\n",
    "    k=10,\n",
    "    keep_unknown_sources=False,    \n",
    ")\n",
    "\n",
    "val_acc_rest, val_acc_clean_rest, test_acc_rest, test_acc_clean_rest = run_experiment(\n",
    "    dataset = 'restaurant',\n",
    "    target = 'city',\n",
    "    k=10,\n",
    "    keep_unknown_sources=True,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b51e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task          retr (val)    retr+clean (val)    GPT-3 0-shot (test)    retr (test)    retr+clean (test)\n",
      "----------  ------------  ------------------  ---------------------  -------------  -------------------\n",
      "buy             0.888889            0.923077                  0.846       0.876923             0.892308\n",
      "restaurant      0.717949            0.762821                  0.709       0.77907              0.790698\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([\n",
    " ('buy', val_acc_buy, val_acc_clean_buy, 0.846, test_acc_buy, test_acc_clean_buy),\n",
    " ('restaurant', val_acc_rest, val_acc_clean_rest, 0.709, test_acc_rest, test_acc_clean_rest),       \n",
    "], headers=['task', 'retr (val)', 'retr+clean (val)', 'GPT-3 0-shot (test)', \n",
    "            'retr (test)', 'retr+clean (test)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da4583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
